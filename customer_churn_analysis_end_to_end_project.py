# -*- coding: utf-8 -*-
"""Customer Churn Analysis_end to end project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18R3MvMkFt_YkDbLL0xd4xYzUqQqPJzmS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.ticker as mtick
import matplotlib.pyplot as plt
# %matplotlib inline

#necessary modules for model building

import pandas as pd
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from imblearn.combine import SMOTEENN

#importing dataset from google drive
data = pd.read_csv('/content/drive/MyDrive/MLProject-ChurnPrediction-main/WA_Fn-UseC_-Telco-Customer-Churn.csv')

for col in data:
    print(col)
    print(data[col].unique())

#observing top 5 values
data.head()

#checking the shape of data-set (rows,columns)
data.shape

data.columns #checking the column names of the dataset

data['Churn'].value_counts() # number of yes and No values in dataset

data.nunique() # number of unique values in each column

data.info() # data type of each columns

"""##### Though our dataset is not having any missing value as observed, but if in case we have any missing value than in order to deal with missing value we can do following techniques.
- If missing values are less,than we can drop those rows.
-If missing values are less, than we can use data imputation techniques like using mean,mode or median or regression to impute tha values in missing areas.
-If the missing values are high in number and the column is not having that much influence in output than we can drop off that column(like atleast 30% of missing values, and in this case domain knowledge is important).
-There are also many other data imputation techniques which can be used accordingly
"""

data.describe() # checking the descriptive statisctis of each numeric variables

"""- Senior citizen is integer but categorical in nature with values 1 and 0 so percentile is not beneficial.
- Tenure values are in number of months, 75 % customers are having tenure less than 55 months.
- Average tenure is 32 months.
- Average monthly charge is 64.76 USD and 75% customers pays more than 89.85 USD per month.
"""

data[data['Churn']=='Yes']

data.dtypes

"""### Exploratory data analysis

Plotting no. of Yes and No Churn Values.
"""

data['Churn'].value_counts().plot(kind='bar',figsize=(8,6),color='maroon')
plt.xlabel("Target Variables",labelpad=14,size=14)
plt.yticks(size=10)
plt.ylabel("Count",labelpad=14,size=14)
plt.title("Values of output variables per category")
plt.show()

# churners value in percentage
(data['Churn'].value_counts()/data['Churn'].count())*100

"""- Data is imbalanced in nature with ration 73:27

- As data set is imbalanced in nature, we have to make it balanced in nature.
- In order to make it balanced dataset, we can go for upsampling or downsampling of dataset(It is one the technique to convert unbalanced dataset into balanced dataset).


---

InCase of Down sampling, there's a possibility of loosing usefull information so we will go for Upsampling.

## Data Cleaning
"""

# to perform various EDA techniques and to ensure there are truly no null
 # values, we perform various Cleaning techniques.

## Creating copy of base data for manipulation and processing

data_copy = data.copy()

# As columns like total charges should be specifically numeric thus performing operation

data_copy['TotalCharges']=pd.to_numeric(data_copy.TotalCharges, errors='coerce')
data_copy.isnull().sum()

"""**OBSERVATION :** Even though in object format Total Charges Column wasn't giving any nul values but as we converted it into numeric format.It gave Null value count to 11.
That is also one of many reasons we have to convert object data type to accurate data type.

### Also since the number of none value columns is very low, so it is safe to drop these columns
"""

data_copy.loc[data_copy['TotalCharges'].isnull()==True]

data_copy.dropna(how='any',inplace=True) # removing the missing values

# creating the bin(range) out of the tenures of customer
 # for
 # getting the maximum values of tenure column
print(data_copy['tenure'].max())

# grouping the labels into bins of 12 months
labels = ["{0}-{1}".format(i,i+11) for i in range(1,72,12)]
data_copy['tenure_group'] = pd.cut(data_copy.tenure,range(1,80,12),right=False,labels=labels)

data_copy['tenure_group'].value_counts()

#droping off irrelevant columns
data_copy.drop(columns=['customerID','tenure'],axis=1,inplace=True)
data_copy.head()

"""## Exploratory Data Analysis

#### Univariate Plotting


"""

# Plotting individual Predictors for churn
for i,predictor in enumerate(data_copy.drop(columns=['Churn','TotalCharges','MonthlyCharges'])):
  plt.figure(i)
  sns.countplot(data=data_copy,x=predictor,hue='Churn')

# as computer only understands numbers so converting output variable churn into integers
data_copy['Churn'] = np.where(data_copy.Churn=='Yes',1,0)

"""#### Converting all the categorical variables into Dummy numerical values"""

data_copy_dummies = pd.get_dummies(data_copy)
data_copy_dummies.head()

#plotting to show relation ship between  monthly charges and TOTAL charges
sns.lmplot(data=data_copy_dummies,x='MonthlyCharges',y='TotalCharges',fit_reg=False)
## total charges directly proportional to monthly charges

# checking churning by monthly charges and total charges
monthly = sns.kdeplot(data_copy_dummies.MonthlyCharges[(data_copy_dummies['Churn']==0) ] ,
                      color='Red',shade=True)
monthly = sns.kdeplot(data_copy_dummies.MonthlyCharges[(data_copy_dummies['Churn'])==1],
                      ax=monthly,color="Blue",shade=True)
monthly.legend(["No Churn","Churn"],loc='upper right')
monthly.set_ylabel("Density")
monthly.set_xlabel("Monthly Charges")
monthly.set_title('monthly charges by churn')

##Showing high churn at high monthly charges

# It is always important to do univariate and multivariate analysis to get the proper insight

# it is also always to draw correleation chart of all the attributes
# with the output variable as it also gives insight that which attribute is
# important for getting the output
plt.figure(figsize=(20,8))
data_copy_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')

"""### here we have used bar plot for showing correleation but we can use conventional heat map correleation chart also.
### It is used as sometime bar plot gives good understandable view also we wanted to make correlation with respect to one feature only and didn't wanted to see correlation among features itself.

*Derived Insight*

- High churner are one having month-to-month contract basis, people having no security deposit, no technical support provided, or if person is in first year of subscription. So it is necessary to hold customer during it's first year of contract.
- Low Churn is seen in case if person is having long-term-contract,if person is associated with more than 5 years.
- Features like Gender,Availability of Phone Service and number of multiple lines have almost NO impact on Customer Churn.

"""

plt.figure(figsize=(12,12))
sns.heatmap(data_copy_dummies.corr(),cmap='copper')

"""### Doing Bivariate Analysis"""

target0 = data_copy.loc[data_copy["Churn"]==0]
target1 = data_copy.loc[data_copy["Churn"]==1]

#function to get plot choosing our feature
def uniplot(df,col,title,hue=None):

  sns.set_style('whitegrid')
  sns.set_context('talk')
  plt.rcParams["axes.labelsize"]=20
  plt.rcParams['axes.titlesize']=22
  plt.rcParams['axes.titlepad']=30

  temp = pd.Series(data=hue)
  fig,ax = plt.subplots()
  width = len(df[col].unique())+7+4*len(temp.unique())
  fig.set_size_inches(width,8)
  plt.xticks(rotation=45)
  plt.yscale('log')
  plt.title(title)
  ax = sns.countplot(data=df, x = col, order=df[col].value_counts().index,hue=hue,palette='bright')

  plt.show()

uniplot(target1,col='Partner',title='Distribution of Gender for Churned Customers',hue='gender')

uniplot(target0,col='Partner',title='Distribution of Gender for Non Churned Customers',hue='gender')

"""- we observed that Churned percentage is high when people have no dependent partner. But male to female ratio is almost similliar."""

uniplot(target1,col='PaymentMethod',title='Distribution of Payment Method for Churned Customers',hue='gender')

"""
### Churned Percentage is very high if the payment method is Electronics Check, irrespective of the gender.

Similliar we can try for other columns."""

# saving preprocessed data as csv file
data_copy_dummies.to_csv('final_data.csv')

"""# **Model Building**"""



# reading the saved csv file,
# because here we are continuing the work in same Jupyter file
# but if we would have been doing it in different file than in that case we would be importing

final_data = pd.read_csv('final_data.csv')

final_data.head(5)

final_data = final_data.drop('Unnamed: 0',axis=1)

final_data.head(5)

# creating x and y variables
x = final_data.drop('Churn',axis=1)
x

y = final_data['Churn']

#train test splitting of data
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)

"""### Decision Tree Classifier(No specific Reason)"""

model_dt = DecisionTreeClassifier(criterion='gini',random_state = 100,max_depth=6,min_samples_leaf=8)

model_dt.fit(x_train,y_train)

y_pred = model_dt.predict(x_test)

y_pred

model_dt.score(x_test,y_pred)

print(classification_report(y_test,y_pred,labels=[0,1])) # classification Report
# from classification report,always look at the minority class
# in our case Churners is the minority class
# by observing various parameters, we conclude that our model is not properly optimised

# one reason can be that our dataset is imbalanced

print(confusion_matrix(y_test,y_pred)) #confusion matrix

#to balancing the dataset, we use smot analysis
# for that we are using smoteen

sm = SMOTEENN()
X_resampled, y_resampled = sm.fit_resample(x,y)

xr_train,xr_test,yr_train,yr_test = train_test_split(X_resampled,y_resampled,test_size=0.2)

model_dt_smote = DecisionTreeClassifier(criterion='gini',random_state = 100,max_depth=6,min_samples_leaf=8)

model_dt_smote.fit(xr_train,yr_train)

y_pred_smote = model_dt_smote.predict(xr_test)

print(classification_report(yr_test,y_pred_smote,labels=[0,1]))
#now looking at miniority result, we are getting much better results

"""
# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(n_estimators=100,criterion='gini',random_state = 100,max_depth=6,min_samples_leaf=8)
model_rf.fit(x_train,y_train)
y_pred_rf = model_rf.predict(x_test)

print(classification_report(y_test,y_pred_rf,labels=[0,1])) #same case as decision tree when imbalanced dataset was taken into consideration

#again creating the smote part for random forest to check if we are getting better result in thiscase

sm = SMOTEENN()
X_resampled, y_resampled = sm.fit_resample(x,y)

xr_train,xr_test,yr_train,yr_test = train_test_split(X_resampled,y_resampled,test_size=0.2)

model_rf_smote = RandomForestClassifier(n_estimators=100,criterion='gini',random_state = 100,max_depth=6,min_samples_leaf=8)

model_rf_smote.fit(xr_train,yr_train)

y_pred_smote_rf = model_rf_smote.predict(xr_test)

print(classification_report(yr_test,y_pred_smote_rf,labels=[0,1]))

print(confusion_matrix(yr_test,y_pred_smote_rf))

"""## Saving the model"""

import pickle

filename = 'model.sav'

pickle.dump(model_rf_smote,open(filename,'wb'))

# calling the model
load_model = pickle.load(open(filename,'rb'))

load_model.score(xr_test,yr_test)

